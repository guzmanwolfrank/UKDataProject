{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n",
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'visualizations' directory for outputs\n",
      "Loading data...\n",
      "\n",
      "Loading 2015 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2015.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2015.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2015.csv\n",
      "Successfully loaded 2015 data\n",
      "\n",
      "Loading 2016 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:28: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2016.csv\n",
      "Could not load casualty data for 2016: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Wolfrank\\\\Desktop\\\\GiGabyte\\\\CodeWolf\\\\UKDataProject\\\\Data\\\\Casualties\\\\Casualties_2016.csv'\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2016.csv\n",
      "\n",
      "Loading 2017 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2017.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n",
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:37: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:46: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2017 data\n",
      "\n",
      "Loading 2018 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2018.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2559387159.py:46: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2018 data\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"13/01/2015\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 226\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis could not be completed due to data loading errors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 205\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accidents_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m     accidents_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccidents_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 86\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(accidents_df)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess the accidents dataframe\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accidents_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     accidents_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccidents_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     accidents_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m accidents_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m     88\u001b[0m     accidents_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(accidents_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n",
      "File \u001b[1;32mc:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32mc:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"13/01/2015\" doesn't match format \"%m/%d/%Y\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = r\"C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\"\n",
    "ACCIDENTS_PATH = os.path.join(BASE_PATH, \"Accidents\")\n",
    "CASUALTIES_PATH = os.path.join(BASE_PATH, \"Casualties\")\n",
    "VEHICLES_PATH = os.path.join(BASE_PATH, \"Vehicles\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create directory for saving visualizations\n",
    "def create_output_directory():\n",
    "    if not os.path.exists('visualizations'):\n",
    "        os.makedirs('visualizations')\n",
    "    print(\"Created 'visualizations' directory for outputs\")\n",
    "\n",
    "# Data loading functions\n",
    "def load_accident_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(ACCIDENTS_PATH, f'Accidents_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load accident data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_casualty_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(CASUALTIES_PATH, f'Casualties_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load casualty data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_vehicle_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(VEHICLES_PATH, f'Vehicles_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load vehicle data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data(years):\n",
    "    accidents_data = []\n",
    "    casualties_data = []\n",
    "    vehicles_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nLoading {year} data...\")\n",
    "        \n",
    "        # Load each dataset\n",
    "        acc_df = load_accident_data(year)\n",
    "        cas_df = load_casualty_data(year)\n",
    "        veh_df = load_vehicle_data(year)\n",
    "        \n",
    "        # Add year column and append if loading successful\n",
    "        if acc_df is not None and cas_df is not None and veh_df is not None:\n",
    "            acc_df['Year'] = year\n",
    "            cas_df['Year'] = year\n",
    "            veh_df['Year'] = year\n",
    "            \n",
    "            accidents_data.append(acc_df)\n",
    "            casualties_data.append(cas_df)\n",
    "            vehicles_data.append(veh_df)\n",
    "            print(f\"Successfully loaded {year} data\")\n",
    "    \n",
    "    # Combine all years if data was loaded\n",
    "    if accidents_data:\n",
    "        return (pd.concat(accidents_data, ignore_index=True),\n",
    "                pd.concat(casualties_data, ignore_index=True),\n",
    "                pd.concat(vehicles_data, ignore_index=True))\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def preprocess_data(accidents_df):\n",
    "    \"\"\"Preprocess the accidents dataframe\"\"\"\n",
    "    if accidents_df is not None:\n",
    "        accidents_df['Date'] = pd.to_datetime(accidents_df['Date'])\n",
    "        accidents_df['Month'] = accidents_df['Date'].dt.month\n",
    "        accidents_df['Hour'] = pd.to_datetime(accidents_df['Time']).dt.hour\n",
    "        accidents_df['Year'] = accidents_df['Date'].dt.year\n",
    "    return accidents_df\n",
    "\n",
    "def create_visualizations(accidents_df, casualties_df, vehicles_df):\n",
    "    \"\"\"Create and save all visualizations\"\"\"\n",
    "    \n",
    "    def save_plot(plt, name):\n",
    "        plt.savefig(f'visualizations/{name}.png', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 1. Yearly Trends\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    yearly_accidents = accidents_df.groupby('Year').size()\n",
    "    sns.lineplot(x=yearly_accidents.index, y=yearly_accidents.values, marker='o')\n",
    "    plt.title('Total Accidents by Year (2015-2018)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Accidents')\n",
    "    save_plot(plt, 'yearly_trends')\n",
    "\n",
    "    # 2. Monthly Patterns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    monthly_accidents = pd.crosstab(accidents_df['Year'], accidents_df['Month'])\n",
    "    sns.heatmap(monthly_accidents, annot=True, fmt='d', cmap='YlOrRd')\n",
    "    plt.title('Accidents by Month and Year')\n",
    "    save_plot(plt, 'monthly_patterns')\n",
    "\n",
    "    # 3. Severity Analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    severity_by_year = pd.crosstab(accidents_df['Year'], accidents_df['Accident_Severity'])\n",
    "    severity_by_year.plot(kind='bar', stacked=True)\n",
    "    plt.title('Accident Severity Distribution by Year')\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, 'severity_analysis')\n",
    "\n",
    "    # 4. Time of Day Analysis\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    time_day = pd.crosstab(accidents_df['Hour'], accidents_df['Day_of_Week'])\n",
    "    sns.heatmap(time_day, cmap='YlOrRd', annot=True, fmt='d')\n",
    "    plt.title('Accidents by Hour and Day of Week')\n",
    "    save_plot(plt, 'time_of_day')\n",
    "\n",
    "    # 5. Weather Conditions\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    weather = pd.crosstab(accidents_df['Weather_Conditions'], \n",
    "                         accidents_df['Accident_Severity'])\n",
    "    weather.plot(kind='bar', stacked=True)\n",
    "    plt.title('Accident Severity by Weather Conditions')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, 'weather_conditions')\n",
    "\n",
    "    # 6. Road Type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=accidents_df, x='Road_Type', hue='Accident_Severity')\n",
    "    plt.title('Accidents by Road Type and Severity')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, 'road_types')\n",
    "\n",
    "    # 7. Casualty Analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data=casualties_df, x='Age_of_Casualty', hue='Casualty_Severity',\n",
    "                multiple=\"stack\", bins=30)\n",
    "    plt.title('Age Distribution of Casualties by Severity')\n",
    "    save_plot(plt, 'casualty_analysis')\n",
    "\n",
    "    # 8. Vehicle Types\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    top_vehicles = vehicles_df['Vehicle_Type'].value_counts().head(10)\n",
    "    sns.barplot(x=top_vehicles.values, y=top_vehicles.index)\n",
    "    plt.title('Top 10 Vehicle Types in Accidents')\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, 'vehicle_types')\n",
    "\n",
    "    # 9. Speed Limit Analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=accidents_df, x='Speed_limit', y='Number_of_Casualties')\n",
    "    plt.title('Casualties by Speed Limit')\n",
    "    save_plot(plt, 'speed_limit')\n",
    "\n",
    "def generate_statistics(accidents_df, casualties_df, vehicles_df):\n",
    "    \"\"\"Generate and save summary statistics\"\"\"\n",
    "    stats = {\n",
    "        'Total Accidents': len(accidents_df),\n",
    "        'Fatal Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Fatal']),\n",
    "        'Serious Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Serious']),\n",
    "        'Slight Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Slight']),\n",
    "        'Total Casualties': len(casualties_df),\n",
    "        'Total Vehicles': len(vehicles_df),\n",
    "        'Average Casualties per Accident': len(casualties_df) / len(accidents_df)\n",
    "    }\n",
    "    \n",
    "    # Save statistics to file\n",
    "    with open('visualizations/statistics.txt', 'w') as f:\n",
    "        f.write(\"Traffic Accident Analysis Statistics\\n\")\n",
    "        f.write(\"==================================\\n\\n\")\n",
    "        for stat, value in stats.items():\n",
    "            if isinstance(value, int):\n",
    "                f.write(f\"{stat}: {value:,}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{stat}: {value:.2f}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    # Create output directory\n",
    "    create_output_directory()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    years = [2015, 2016, 2017, 2018]\n",
    "    accidents_df, casualties_df, vehicles_df = load_all_data(years)\n",
    "    \n",
    "    if accidents_df is not None:\n",
    "        # Preprocess data\n",
    "        print(\"Preprocessing data...\")\n",
    "        accidents_df = preprocess_data(accidents_df)\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(\"Creating visualizations...\")\n",
    "        create_visualizations(accidents_df, casualties_df, vehicles_df)\n",
    "        \n",
    "        # Generate statistics\n",
    "        print(\"Generating statistics...\")\n",
    "        stats = generate_statistics(accidents_df, casualties_df, vehicles_df)\n",
    "        \n",
    "        print(\"\\nAnalysis complete! Results saved in 'visualizations' directory\")\n",
    "        print(\"\\nKey Statistics:\")\n",
    "        for stat, value in stats.items():\n",
    "            if isinstance(value, int):\n",
    "                print(f\"{stat}: {value:,}\")\n",
    "            else:\n",
    "                print(f\"{stat}: {value:.2f}\")\n",
    "    else:\n",
    "        print(\"Analysis could not be completed due to data loading errors\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n",
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'visualizations' directory for outputs\n",
      "Loading data...\n",
      "\n",
      "Loading 2015 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2015.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2015.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2015.csv\n",
      "Successfully loaded 2015 data\n",
      "\n",
      "Loading 2016 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2016.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2016.csv\n",
      "Could not load casualty data for 2016: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Wolfrank\\\\Desktop\\\\GiGabyte\\\\CodeWolf\\\\UKDataProject\\\\Data\\\\Casualties\\\\Casualties_2016.csv'\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:28: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 2017 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2017.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n",
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:37: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:46: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n",
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2017 data\n",
      "\n",
      "Loading 2018 data...\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Accidents\\Accidents_2018.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Casualties\\Casualties_2018.csv\n",
      "Attempting to load: C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\\Vehicles\\Vehicles_2018.csv\n",
      "Successfully loaded 2018 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wolfrank\\AppData\\Local\\Temp\\ipykernel_15824\\2382717461.py:46: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data preprocessing completed successfully\n",
      "Creating visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "c:\\Users\\Wolfrank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All visualizations created successfully\n",
      "Generating statistics...\n",
      "\n",
      "Analysis complete! Results saved in 'visualizations' directory\n",
      "\n",
      "Key Statistics:\n",
      "Total Accidents: 392,673\n",
      "Fatal Accidents: 0\n",
      "Serious Accidents: 0\n",
      "Slight Accidents: 0\n",
      "Total Casualties: 517,779\n",
      "Total Vehicles: 723,180\n",
      "Average Casualties per Accident: 1.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = r\"C:\\Users\\Wolfrank\\Desktop\\GiGabyte\\CodeWolf\\UKDataProject\\Data\"\n",
    "ACCIDENTS_PATH = os.path.join(BASE_PATH, \"Accidents\")\n",
    "CASUALTIES_PATH = os.path.join(BASE_PATH, \"Casualties\")\n",
    "VEHICLES_PATH = os.path.join(BASE_PATH, \"Vehicles\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create directory for saving visualizations\n",
    "def create_output_directory():\n",
    "    if not os.path.exists('visualizations'):\n",
    "        os.makedirs('visualizations')\n",
    "    print(\"Created 'visualizations' directory for outputs\")\n",
    "\n",
    "# Data loading functions\n",
    "def load_accident_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(ACCIDENTS_PATH, f'Accidents_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load accident data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_casualty_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(CASUALTIES_PATH, f'Casualties_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load casualty data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_vehicle_data(year):\n",
    "    try:\n",
    "        file_path = os.path.join(VEHICLES_PATH, f'Vehicles_{year}.csv')\n",
    "        print(f\"Attempting to load: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Could not load vehicle data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_data(years):\n",
    "    accidents_data = []\n",
    "    casualties_data = []\n",
    "    vehicles_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nLoading {year} data...\")\n",
    "        \n",
    "        # Load each dataset\n",
    "        acc_df = load_accident_data(year)\n",
    "        cas_df = load_casualty_data(year)\n",
    "        veh_df = load_vehicle_data(year)\n",
    "        \n",
    "        # Add year column and append if loading successful\n",
    "        if acc_df is not None and cas_df is not None and veh_df is not None:\n",
    "            acc_df['Year'] = year\n",
    "            cas_df['Year'] = year\n",
    "            veh_df['Year'] = year\n",
    "            \n",
    "            accidents_data.append(acc_df)\n",
    "            casualties_data.append(cas_df)\n",
    "            vehicles_data.append(veh_df)\n",
    "            print(f\"Successfully loaded {year} data\")\n",
    "    \n",
    "    # Combine all years if data was loaded\n",
    "    if accidents_data:\n",
    "        return (pd.concat(accidents_data, ignore_index=True),\n",
    "                pd.concat(casualties_data, ignore_index=True),\n",
    "                pd.concat(vehicles_data, ignore_index=True))\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def preprocess_data(accidents_df):\n",
    "    \"\"\"Preprocess the accidents dataframe with UK date format\"\"\"\n",
    "    if accidents_df is not None:\n",
    "        try:\n",
    "            # Convert dates using UK format (day first)\n",
    "            accidents_df['Date'] = pd.to_datetime(accidents_df['Date'], format='%d/%m/%Y', dayfirst=True)\n",
    "            accidents_df['Month'] = accidents_df['Date'].dt.month\n",
    "            \n",
    "            # Convert time - check if Time column exists and handle accordingly\n",
    "            if 'Time' in accidents_df.columns:\n",
    "                try:\n",
    "                    accidents_df['Hour'] = pd.to_datetime(accidents_df['Time'], format='%H:%M').dt.hour\n",
    "                except ValueError:\n",
    "                    print(\"Warning: Time format different than expected. Using basic hour extraction.\")\n",
    "                    accidents_df['Hour'] = accidents_df['Time'].str.split(':').str[0].astype(int)\n",
    "            \n",
    "            accidents_df['Year'] = accidents_df['Date'].dt.year\n",
    "            print(\"Data preprocessing completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during preprocessing: {e}\")\n",
    "            return None\n",
    "    return accidents_df\n",
    "\n",
    "def create_visualizations(accidents_df, casualties_df, vehicles_df):\n",
    "    \"\"\"Create and save all visualizations\"\"\"\n",
    "    \n",
    "    def save_plot(plt, name):\n",
    "        plt.savefig(f'visualizations/{name}.png', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    try:\n",
    "        # 1. Yearly Trends\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        yearly_accidents = accidents_df.groupby('Year').size()\n",
    "        sns.lineplot(x=yearly_accidents.index, y=yearly_accidents.values, marker='o')\n",
    "        plt.title('Total Accidents by Year (2015-2018)')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Accidents')\n",
    "        save_plot(plt, 'yearly_trends')\n",
    "\n",
    "        # 2. Monthly Patterns\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        monthly_accidents = pd.crosstab(accidents_df['Year'], accidents_df['Month'])\n",
    "        sns.heatmap(monthly_accidents, annot=True, fmt='d', cmap='YlOrRd')\n",
    "        plt.title('Accidents by Month and Year')\n",
    "        save_plot(plt, 'monthly_patterns')\n",
    "\n",
    "        # 3. Severity Analysis\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        severity_by_year = pd.crosstab(accidents_df['Year'], accidents_df['Accident_Severity'])\n",
    "        severity_by_year.plot(kind='bar', stacked=True)\n",
    "        plt.title('Accident Severity Distribution by Year')\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, 'severity_analysis')\n",
    "\n",
    "        # 4. Time of Day Analysis\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        time_day = pd.crosstab(accidents_df['Hour'], accidents_df['Day_of_Week'])\n",
    "        sns.heatmap(time_day, cmap='YlOrRd', annot=True, fmt='d')\n",
    "        plt.title('Accidents by Hour and Day of Week')\n",
    "        save_plot(plt, 'time_of_day')\n",
    "\n",
    "        # 5. Weather Conditions\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        weather = pd.crosstab(accidents_df['Weather_Conditions'], \n",
    "                            accidents_df['Accident_Severity'])\n",
    "        weather.plot(kind='bar', stacked=True)\n",
    "        plt.title('Accident Severity by Weather Conditions')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, 'weather_conditions')\n",
    "\n",
    "        # 6. Road Type\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.countplot(data=accidents_df, x='Road_Type', hue='Accident_Severity')\n",
    "        plt.title('Accidents by Road Type and Severity')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, 'road_types')\n",
    "\n",
    "        # 7. Casualty Analysis\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(data=casualties_df, x='Age_of_Casualty', hue='Casualty_Severity',\n",
    "                    multiple=\"stack\", bins=30)\n",
    "        plt.title('Age Distribution of Casualties by Severity')\n",
    "        save_plot(plt, 'casualty_analysis')\n",
    "\n",
    "        # 8. Vehicle Types\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        top_vehicles = vehicles_df['Vehicle_Type'].value_counts().head(10)\n",
    "        sns.barplot(x=top_vehicles.values, y=top_vehicles.index)\n",
    "        plt.title('Top 10 Vehicle Types in Accidents')\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, 'vehicle_types')\n",
    "\n",
    "        # 9. Speed Limit Analysis\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=accidents_df, x='Speed_limit', y='Number_of_Casualties')\n",
    "        plt.title('Casualties by Speed Limit')\n",
    "        save_plot(plt, 'speed_limit')\n",
    "\n",
    "        print(\"All visualizations created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualizations: {e}\")\n",
    "\n",
    "def generate_statistics(accidents_df, casualties_df, vehicles_df):\n",
    "    \"\"\"Generate and save summary statistics\"\"\"\n",
    "    try:\n",
    "        stats = {\n",
    "            'Total Accidents': len(accidents_df),\n",
    "            'Fatal Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Fatal']),\n",
    "            'Serious Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Serious']),\n",
    "            'Slight Accidents': len(accidents_df[accidents_df['Accident_Severity'] == 'Slight']),\n",
    "            'Total Casualties': len(casualties_df),\n",
    "            'Total Vehicles': len(vehicles_df),\n",
    "            'Average Casualties per Accident': len(casualties_df) / len(accidents_df)\n",
    "        }\n",
    "        \n",
    "        # Save statistics to file\n",
    "        with open('visualizations/statistics.txt', 'w') as f:\n",
    "            f.write(\"Traffic Accident Analysis Statistics\\n\")\n",
    "            f.write(\"==================================\\n\\n\")\n",
    "            for stat, value in stats.items():\n",
    "                if isinstance(value, int):\n",
    "                    f.write(f\"{stat}: {value:,}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{stat}: {value:.2f}\\n\")\n",
    "        \n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating statistics: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Create output directory\n",
    "        create_output_directory()\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        years = [2015, 2016, 2017, 2018]\n",
    "        accidents_df, casualties_df, vehicles_df = load_all_data(years)\n",
    "        \n",
    "        if accidents_df is not None:\n",
    "            # Preprocess data\n",
    "            print(\"Preprocessing data...\")\n",
    "            accidents_df = preprocess_data(accidents_df)\n",
    "            \n",
    "            if accidents_df is not None:\n",
    "                # Create visualizations\n",
    "                print(\"Creating visualizations...\")\n",
    "                create_visualizations(accidents_df, casualties_df, vehicles_df)\n",
    "                \n",
    "                # Generate statistics\n",
    "                print(\"Generating statistics...\")\n",
    "                stats = generate_statistics(accidents_df, casualties_df, vehicles_df)\n",
    "                \n",
    "                if stats:\n",
    "                    print(\"\\nAnalysis complete! Results saved in 'visualizations' directory\")\n",
    "                    print(\"\\nKey Statistics:\")\n",
    "                    for stat, value in stats.items():\n",
    "                        if isinstance(value, int):\n",
    "                            print(f\"{stat}: {value:,}\")\n",
    "                        else:\n",
    "                            print(f\"{stat}: {value:.2f}\")\n",
    "            else:\n",
    "                print(\"Error during data preprocessing\")\n",
    "        else:\n",
    "            print(\"Analysis could not be completed due to data loading errors\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
